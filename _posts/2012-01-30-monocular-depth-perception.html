---
layout: post
status: published
published: true
title: Monocular Depth Perception
author:
  display_name: Judebert
  login: judebert
  email: judebert@judebert.com
author_login: judebert
author_email: judebert@judebert.com
date: 2012-01-30 15:48:26 UTC
permalink: "/progress/archives/423-Monocular-Depth-Perception.html"
categories:
- The Attic
tags: []
comments: []
excerpt_separator: <a id="extended"></a>
---
<p>
I was thinking about the depth perception techniques we use for automated vehicles.  The one that one the DARPA Grand Challenge used a laser rangefinder, as well as a camera; many of the contestants used lasers, radar, or a combination of the two.
</p><p>
It's generally well-known that human depth perception is provided by two eyes at an offset (binocular vision).  There was one Grand Challenge car that used two cameras as its sole sensing equipment; I liked that solution, not only because of its simplicity, but because it modeled human vision.  (They finished second, as I recall.)
</p><p>
However, wouldn't it be simpler to do it with a single camera?
</p><a id="extended"></a><p>
Monocular depth perception has a few tricks up its sleeve.  Humans with a single available eye take cues from occlusion (near things covering up far things), motion (near things appear to go by faster than far things), perspective... we can even move a little from side to side, providing a sort of "virtual" eye at each extreme.
</p><p>
Moving the camera back and forth on an automated car would be a little extreme, in my opinion.  You'd have to oscillate <a href="http://en.wikipedia.org/wiki/Pupillary_distance">from 41 to 73 mm</a> to mimic the human eye.
</p><p>
But then I had an idea: why not do it with focus depth?
</p><p>
A camera could rapidly switch from one focus to another.  Comparing the two images should give you a pretty good idea of how far each item is from the camera.  Heck, you should know what depth you expect to be focusing on; anything that's sharp is at that depth.
</p><p>
I thought I'd come up with something revolutionary, but -- like many of my ideas -- somebody else got there first.  <a href="http://en.wikipedia.org/wiki/Depth_perception#Monocular_cues">Wikipedia says there are several depth estimation algorithms based on defocus and blurring</a>.  
</p><p>
Apparently, there are even jumping spiders that use depth-of-field blurring to estimate distances.  I wasn't just beaten out by man, but by nature, too.
</p><p>
I'm still keeping it in the back of my mind.  If you know the depth you're focusing on, you should be able to estimate distance without a lot of image processing.  A simple edge-detection algorithm should do it.  Better yet, you should be able to focus over a <em>long</em> range with a relatively small movement in the camera mechanism.  
</p><p>
It could still be useful.
</p>
